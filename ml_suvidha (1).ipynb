{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOpcJxnzJahXybjhZ6ZW63e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4jXE87fhaUb","executionInfo":{"status":"ok","timestamp":1694329405870,"user_tz":-240,"elapsed":16450,"user":{"displayName":"Anjali Vengatesan","userId":"10410964075097892456"}},"outputId":"d6349a42-95d7-46df-8c69-6c2637b96e40"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","!pip install rouge\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, Concatenate\n","from tensorflow.keras.optimizers import Adam\n","!pip install nltk\n","import nltk\n","nltk.download('punkt')\n","\n","from rouge import Rouge\n","\n","\n","# Define hyperparameters\n","vocab_size = 10000\n","embedding_dim = 300\n","hidden_dim = 256\n","dropout_rate = 0.2\n","learning_rate = 0.001\n","batch_size = 32\n","num_epochs = 10\n"]},{"cell_type":"code","source":["# Load your dataset\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Define the number of examples to process\n","num_examples = 100  # Change this to the desired number of examples\n","\n","# Load your dataset from Google Drive (only a subset)\n","file_path = '/content/drive/My Drive/train.csv'\n","train_data = pd.read_csv(file_path, nrows=num_examples)\n","file_path1 = '/content/drive/My Drive/test.csv'\n","test_data = pd.read_csv(file_path1, nrows=num_examples)\n","file_path2 = '/content/drive/My Drive/validation.csv'\n","val_data = pd.read_csv(file_path2, nrows=num_examples)\n","\n","# Clean and preprocess the text\n","def preprocess_text(text):\n","    text = text.lower()  # Convert to lowercase\n","    words = word_tokenize(text)  # Tokenize using NLTK\n","    cleaned_text = ' '.join(words)\n","    return cleaned_text\n","\n","# Apply preprocessing to each dataset\n","def preprocess_dataset(dataset):\n","    dataset['cleaned_text'] = dataset['article'].apply(preprocess_text)\n","    dataset['cleaned_highlights'] = dataset['highlights'].apply(preprocess_text)\n","\n","# Preprocess the train dataset\n","preprocess_dataset(train_data)\n","\n","# Preprocess the validation dataset\n","preprocess_dataset(val_data)\n","\n","# Preprocess the test dataset\n","preprocess_dataset(test_data)\n","\n","# Save the preprocessed data to new CSV files\n","train_data.to_csv('preprocessed_train.csv', index=False)\n","val_data.to_csv('preprocessed_validation.csv', index=False)\n","test_data.to_csv('preprocessed_test.csv', index=False)\n","\n","# Print the first few rows of each dataset to verify\n","print(train_data.head())\n","print(val_data.head())\n","print(test_data.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MN6F4hI5hc2L","executionInfo":{"status":"ok","timestamp":1694329431905,"user_tz":-240,"elapsed":26044,"user":{"displayName":"Anjali Vengatesan","userId":"10410964075097892456"}},"outputId":"6f07bcdc-0f27-4443-b9d2-fefa2ab17a2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","                                         id  \\\n","0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n","1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n","2  00027e965c8264c35cc1bc55556db388da82b07f   \n","3  0002c17436637c4fe1837c935c04de47adb18e9a   \n","4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n","\n","                                             article  \\\n","0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n","1  (CNN) -- Ralph Mata was an internal affairs li...   \n","2  A drunk driver who killed a young woman in a h...   \n","3  (CNN) -- With a breezy sweep of his pen Presid...   \n","4  Fleetwood are the only team still to have a 10...   \n","\n","                                          highlights  \\\n","0  Bishop John Folda, of North Dakota, is taking ...   \n","1  Criminal complaint: Cop used his role to help ...   \n","2  Craig Eccleston-Todd, 27, had drunk at least t...   \n","3  Nina dos Santos says Europe must be ready to a...   \n","4  Fleetwood top of League One after 2-0 win at S...   \n","\n","                                        cleaned_text  \\\n","0  by . associated press . published : . 14:11 es...   \n","1  ( cnn ) -- ralph mata was an internal affairs ...   \n","2  a drunk driver who killed a young woman in a h...   \n","3  ( cnn ) -- with a breezy sweep of his pen pres...   \n","4  fleetwood are the only team still to have a 10...   \n","\n","                                  cleaned_highlights  \n","0  bishop john folda , of north dakota , is takin...  \n","1  criminal complaint : cop used his role to help...  \n","2  craig eccleston-todd , 27 , had drunk at least...  \n","3  nina dos santos says europe must be ready to a...  \n","4  fleetwood top of league one after 2-0 win at s...  \n","                                         id  \\\n","0  61df4979ac5fcc2b71be46ed6fe5a46ce7f071c3   \n","1  21c0bd69b7e7df285c3d1b1cf56d4da925980a68   \n","2  56f340189cd128194b2e7cb8c26bb900e3a848b4   \n","3  00a665151b89a53e5a08a389df8334f4106494c2   \n","4  9f6fbd3c497c4d28879bebebea220884f03eb41a   \n","\n","                                             article  \\\n","0  Sally Forrest, an actress-dancer who graced th...   \n","1  A middle-school teacher in China has inked hun...   \n","2  A man convicted of killing the father and sist...   \n","3  Avid rugby fan Prince Harry could barely watch...   \n","4  A Triple M Radio producer has been inundated w...   \n","\n","                                          highlights  \\\n","0  Sally Forrest, an actress-dancer who graced th...   \n","1  Works include pictures of Presidential Palace ...   \n","2  Iftekhar Murtaza, 29, was convicted a year ago...   \n","3  Prince Harry in attendance for England's crunc...   \n","4  Nick Slater's colleagues uploaded a picture to...   \n","\n","                                        cleaned_text  \\\n","0  sally forrest , an actress-dancer who graced t...   \n","1  a middle-school teacher in china has inked hun...   \n","2  a man convicted of killing the father and sist...   \n","3  avid rugby fan prince harry could barely watch...   \n","4  a triple m radio producer has been inundated w...   \n","\n","                                  cleaned_highlights  \n","0  sally forrest , an actress-dancer who graced t...  \n","1  works include pictures of presidential palace ...  \n","2  iftekhar murtaza , 29 , was convicted a year a...  \n","3  prince harry in attendance for england 's crun...  \n","4  nick slater 's colleagues uploaded a picture t...  \n","                                         id  \\\n","0  92c514c913c0bdfe25341af9fd72b29db544099b   \n","1  2003841c7dc0e7c5b1a248f9cd536d727f27a45a   \n","2  91b7d2311527f5c2b63a65ca98d21d9c92485149   \n","3  caabf9cbdf96eb1410295a673e953d304391bfbb   \n","4  3da746a7d9afcaa659088c8366ef6347fe6b53ea   \n","\n","                                             article  \\\n","0  Ever noticed how plane seats appear to be gett...   \n","1  A drunk teenage boy had to be rescued by secur...   \n","2  Dougie Freedman is on the verge of agreeing a ...   \n","3  Liverpool target Neto is also wanted by PSG an...   \n","4  Bruce Jenner will break his silence in a two-h...   \n","\n","                                          highlights  \\\n","0  Experts question if  packed out planes are put...   \n","1  Drunk teenage boy climbed into lion enclosure ...   \n","2  Nottingham Forest are close to extending Dougi...   \n","3  Fiorentina goalkeeper Neto has been linked wit...   \n","4  Tell-all interview with the reality TV star, 6...   \n","\n","                                        cleaned_text  \\\n","0  ever noticed how plane seats appear to be gett...   \n","1  a drunk teenage boy had to be rescued by secur...   \n","2  dougie freedman is on the verge of agreeing a ...   \n","3  liverpool target neto is also wanted by psg an...   \n","4  bruce jenner will break his silence in a two-h...   \n","\n","                                  cleaned_highlights  \n","0  experts question if packed out planes are putt...  \n","1  drunk teenage boy climbed into lion enclosure ...  \n","2  nottingham forest are close to extending dougi...  \n","3  fiorentina goalkeeper neto has been linked wit...  \n","4  tell-all interview with the reality tv star , ...  \n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention, Concatenate\n","from tensorflow.keras.optimizers import Adam\n","from nltk.translate.bleu_score import sentence_bleu\n","from rouge import Rouge\n","\n","# Load your preprocessed data (assuming CSV files are available)\n","# Use a very small subset of the data by specifying the number of examples\n","num_examples = 10  # Change this to the desired number of examples\n","train_data = pd.read_csv('preprocessed_train.csv', nrows=num_examples)\n","val_data = pd.read_csv('preprocessed_validation.csv', nrows=num_examples)\n","test_data = pd.read_csv('preprocessed_test.csv', nrows=num_examples)\n","\n","# Define hyperparameters for a very small dataset\n","vocab_size = 2000  # Reduce vocabulary size\n","embedding_dim = 50  # Reduce embedding dimension\n","hidden_dim = 64  # Reduce hidden dimension\n","dropout_rate = 0.2\n","learning_rate = 0.001\n","batch_size = 4  # Reduce batch size for a very small dataset\n","num_epochs = 5  # You can reduce the number of epochs as needed\n","\n","\n","\n","# Define the model with adjusted hyperparameters\n","encoder_inputs = Input(shape=(None,))\n","encoder_embed = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n","encoder_lstm = LSTM(hidden_dim, dropout=dropout_rate, return_sequences=True, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_embed)\n","\n","decoder_inputs = Input(shape=(None,))\n","decoder_embed = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n","decoder_lstm = LSTM(hidden_dim, dropout=dropout_rate, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_embed, initial_state=[state_h, state_c])\n","\n","attention = Attention(use_scale=True, dropout=dropout_rate)\n","context_vector = attention([decoder_outputs, encoder_outputs])\n","\n","decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_outputs])\n","\n","pointer_probs = Dense(1, activation='sigmoid')(decoder_combined_context)\n","generator_probs = Dense(vocab_size, activation='softmax')(decoder_combined_context)\n","\n","final_probs = pointer_probs * generator_probs\n","\n","model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=final_probs)\n","\n","optimizer = Adam(learning_rate)\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n","\n"],"metadata":{"id":"pdCmdOT_htzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from nltk.translate.bleu_score import corpus_bleu\n","from rouge import Rouge\n","import random\n","\n","# Load your test data\n","val_data = pd.read_csv('preprocessed_validation.csv')\n","test_data = pd.read_csv('preprocessed_test.csv')\n","\n","# Define your tokenizer and max text length (use the same values as during training)\n","# tokenizer = ...  # Define your tokenizer here\n","max_text_length = 100\n","max_summary_length = 50\n","\n","# Load your trained model\n","model = tf.keras.models.load_model('my_trained_summarization_model.h5')\n","\n","# Function to generate summaries\n","def generate_summary(input_sequence):\n","    if input_sequence is None or not isinstance(input_sequence, str) or len(input_sequence) == 0:\n","        return \"\"  # Return an empty summary if input_sequence is None, not a string, or an empty string\n","\n","    # Tokenize the input sequence\n","    input_sequence = tokenizer.texts_to_sequences([input_sequence])\n","\n","    # If '<start>' token is in the word_index, use it; otherwise, use 0 as a placeholder\n","    if '<start>' in tokenizer.word_index:\n","        start_token = tokenizer.word_index['<start>']\n","    else:\n","        start_token = 0\n","\n","    # If '<end>' token is in the word_index, use it; otherwise, use 0 as a placeholder\n","    if '<end>' in tokenizer.word_index:\n","        end_token = tokenizer.word_index['<end>']\n","    else:\n","        end_token = 0\n","\n","    # Pad the input sequence\n","    if isinstance(input_sequence, str) and len(input_sequence) > 0:\n","        input_sequence = pad_sequences([input_sequence], maxlen=max_text_length, padding='post')\n","    else:\n","        input_sequence = np.zeros((1, max_text_length))  # Create a padded sequence of zeros for empty or non-string inputs\n","\n","    # Initialize the target sequence with the start token\n","    target_sequence = np.zeros((1, max_summary_length))\n","    target_sequence[0, 0] = start_token\n","\n","    # Generate the summary one word at a time\n","    for i in range(1, max_summary_length):\n","        # Predict the next word\n","        predictions = model.predict([input_sequence, target_sequence], batch_size=10000, verbose=0)\n","\n","        # Sample the word with the highest probability\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        target_sequence[0, i] = sampled_token_index\n","\n","        # If the generated word is the end token or the placeholder, stop generating\n","        if sampled_token_index == end_token or sampled_token_index == 0:\n","            break\n","\n","    # Convert the generated sequence back to text\n","    generated_summary = \" \".join([tokenizer.index_word.get(int(token), '<unk>') for token in target_sequence[0] if token != 0])\n","    return generated_summary  # Return the generated summary\n","\n","# Number of random samples to evaluate (adjust as needed)\n","num_samples_to_evaluate = 100\n","\n","# Randomly select a subset of samples for evaluation\n","sample_indices = random.sample(range(len(test_data)), num_samples_to_evaluate)\n","sample_data = test_data.iloc[sample_indices]\n","\n","# Lists to store generated and reference summaries\n","generated_summaries = []\n","reference_summaries = []\n","\n","# Generate and collect summaries\n","for input_seq in sample_data['article']:\n","    if isinstance(input_seq, str) and len(input_seq) > 0:\n","        generated_summary = generate_summary(input_seq)\n","        generated_summaries.append(generated_summary)\n","        reference_summaries.append(sample_data.loc[sample_data['article'] == input_seq, 'cleaned_highlights'].values[0])\n","\n","# Calculate BLEU scores\n","bleu_scores = corpus_bleu(reference_summaries, generated_summaries)\n","\n","# Calculate ROUGE scores\n","rouge = Rouge()\n","rouge_scores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n","\n","\n","# Print the evaluation results\n","print(\"Average BLEU Score:\", np.mean(bleu_scores))\n","print(\"ROUGE Scores:\", rouge_scores)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tud7TBSt-LbX","executionInfo":{"status":"ok","timestamp":1694333515864,"user_tz":-240,"elapsed":314513,"user":{"displayName":"Anjali Vengatesan","userId":"10410964075097892456"}},"outputId":"776df9c9-3c99-478e-d845-967417c727c5"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"output_type":"stream","name":"stdout","text":["Average BLEU Score: 6.164164564713577e-232\n","ROUGE Scores: {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n"]}]}]}